"""
Video Processor Service - FastAPI Application
Handles video frame extraction and Gemini Vision analysis for recipe extraction
"""
from fastapi import FastAPI, UploadFile, File, Form, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from contextlib import asynccontextmanager
import os
import tempfile
import shutil
import logging
import asyncio
from pathlib import Path
from typing import Dict, Any

from .extractor import extract_key_frames
from .analyzer import analyze_recipe_from_frames
from .pipeline import analyze_recipe_from_url
from .video_utils import get_video_metadata
from .config import (
    ALLOWED_ORIGINS,
    GEMINI_API_KEY,
    PROCESSOR_MODE,
    BACKEND_API_URL,
    POLL_INTERVAL_MS
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    Application lifespan manager
    Start active mode worker if configured
    """
    # Startup
    if PROCESSOR_MODE == "active":
        logger.info(f"[Active Mode] Starting in ACTIVE mode")
        logger.info(f"[Active Mode] Will poll {BACKEND_API_URL} every {POLL_INTERVAL_MS}ms")
        # Start active mode worker as background task
        task = asyncio.create_task(active_mode_worker())
    else:
        logger.info(f"[Passive Mode] Starting in PASSIVE mode (default)")
        logger.info(f"[Passive Mode] Waiting for requests on /analyze and /analyze-from-url endpoints")
        task = None

    yield

    # Shutdown
    if task:
        task.cancel()
        try:
            await task
        except asyncio.CancelledError:
            pass


app = FastAPI(
    title="愛煮小幫手 Video Processor",
    description="Video frame extraction and Gemini Vision analysis service",
    version="2.0.0",
    lifespan=lifespan
)

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS.split(","),
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "service": "video-processor"}


@app.get("/ready")
async def readiness_check():
    """Readiness check endpoint with dependency validation"""
    checks = {}

    # Check FFmpeg availability
    try:
        import subprocess
        subprocess.run(['ffmpeg', '-version'], capture_output=True, check=True)
        checks['ffmpeg'] = 'ok'
    except Exception as e:
        checks['ffmpeg'] = f'error: {str(e)}'

    # Check Gemini API key
    checks['gemini_api_key'] = 'ok' if GEMINI_API_KEY else 'missing'

    # Overall status
    all_ok = all(v == 'ok' for v in checks.values())
    status = "ready" if all_ok else "not_ready"

    return {"status": status, "checks": checks}


@app.get("/")
async def root():
    """Root endpoint"""
    return {"message": "愛煮小幫手 Video Processor Service (Gemini Vision)"}


@app.post("/analyze")
async def analyze_video(video: UploadFile = File(...)):
    """
    Analyze cooking video using Gemini Vision API

    Process:
    1. Save uploaded video to temp file
    2. Extract frames at 1fps using FFmpeg
    3. Select 12 key frames (evenly distributed)
    4. Analyze with Gemini Vision API
    5. Return structured recipe JSON with cost metadata
    6. Cleanup temp files

    Args:
        video: Uploaded video file

    Returns:
        Recipe JSON with name, ingredients, steps, tags, and metadata (tokens, video info)
    """
    temp_dir = None
    video_path = None

    try:
        # Create temporary directory
        temp_dir = tempfile.mkdtemp(prefix='recipeai_')
        logger.info(f"Created temp dir: {temp_dir}")

        # Save uploaded video
        video_path = os.path.join(temp_dir, f"video_{os.urandom(4).hex()}.mp4")
        with open(video_path, 'wb') as f:
            shutil.copyfileobj(video.file, f)
        logger.info(f"Saved video: {video_path}")

        # Collect video metadata
        # Get video metadata using FFmpeg
        metadata = get_video_metadata(video_path)
        video_file_size = metadata['size']
        video_duration = metadata['duration']
        logger.info(f"Video file size: {video_file_size} bytes")
        logger.info(f"Video duration: {video_duration}s")

        # Extract key frames
        frames_dir = os.path.join(temp_dir, 'frames')
        all_frames = extract_key_frames(video_path, frames_dir, count=12)
        logger.info(f"Extracted {len(all_frames)} frames")

        if not all_frames:
            raise HTTPException(
                status_code=400,
                detail="No frames could be extracted from video"
            )

        # Analyze with Gemini Vision
        analysis_result = analyze_recipe_from_frames(all_frames)
        recipe_data = analysis_result['recipe']
        usage_metadata = analysis_result['usage_metadata']

        logger.info(f"Analysis complete: {recipe_data.get('name', 'Unknown')}")
        logger.info(f"Token usage: {usage_metadata['total_tokens']} tokens")

        # Return recipe with metadata
        return {
            **recipe_data,
            'metadata': {
                'gemini_tokens': usage_metadata,
                'video_info': {
                    'duration_seconds': video_duration,
                    'file_size_bytes': video_file_size,
                    'frames_extracted': len(all_frames),
                    'frames_analyzed': len(all_frames)
                }
            }
        }

    except Exception as e:
        logger.error(f"Video analysis failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Video analysis failed: {str(e)}"
        )

    finally:
        # Cleanup temp files
        if temp_dir and os.path.exists(temp_dir):
            try:
                shutil.rmtree(temp_dir)
                logger.info(f"Cleaned up temp dir: {temp_dir}")
            except Exception as e:
                logger.warning(f"Failed to cleanup {temp_dir}: {e}")


@app.post("/analyze-from-url")
async def analyze_video_from_url(video_url: str = Form(...)):
    """
    Analyze cooking video from URL using Gemini Vision API

    Process:
    1. Download video from URL (YouTube, Instagram, Facebook, etc.)
    2. Extract frames at 1fps using FFmpeg
    3. Select 12 key frames (evenly distributed)
    4. Analyze with Gemini Vision API
    5. Return structured recipe JSON
    6. Cleanup temp files

    Args:
        video_url: Video URL (supports YouTube, Instagram, Facebook, etc.)

    Returns:
        Recipe JSON with name, ingredients, steps, tags, completeness status
    """
    try:
        logger.info(f"Analyzing video from URL: {video_url}")

        # Use pipeline to handle download -> extract -> analyze
        recipe_data = analyze_recipe_from_url(video_url, cleanup=True)

        logger.info(f"Analysis complete: {recipe_data.get('name', 'Unknown')}")
        return recipe_data

    except ValueError as e:
        logger.error(f"Invalid input: {str(e)}")
        raise HTTPException(
            status_code=400,
            detail=str(e)
        )
    except Exception as e:
        logger.error(f"Video analysis from URL failed: {str(e)}", exc_info=True)
        raise HTTPException(
            status_code=500,
            detail=f"Video analysis failed: {str(e)}"
        )


# ============================================================================
# Active Mode Worker Logic
# ============================================================================

async def process_job(job: Dict[str, Any]) -> Dict[str, Any]:
    """
    Process a single failed analysis job

    Args:
        job: Job data from backend API

    Returns:
        Result dict with recipe and metadata
    """
    job_id = job['job_id']
    video_url = job.get('video_url')
    video_file_id = job.get('video_file_id')

    logger.info(f"[Active Mode] Processing job {job_id}")

    # Choose processing method based on input type
    if video_url:
        logger.info(f"[Active Mode] Analyzing from URL: {video_url}")
        # Use existing analyze_recipe_from_url function
        recipe_data = analyze_recipe_from_url(video_url, cleanup=True)

        return {
            'recipe': recipe_data,
            'metadata': recipe_data.get('metadata')
        }
    elif video_file_id:
        # TODO: Implement LINE video download if needed
        raise NotImplementedError("LINE video_file_id processing not yet implemented in active mode")
    else:
        raise ValueError("No video_url or video_file_id provided")


async def active_mode_worker():
    """
    Active mode worker that polls backend API for failed jobs
    Runs indefinitely until application shutdown
    """
    import httpx

    poll_interval_seconds = POLL_INTERVAL_MS / 1000
    logger.info(f"[Active Mode] Starting active worker (poll interval: {poll_interval_seconds}s)")
    logger.info(f"[Active Mode] Backend API: {BACKEND_API_URL}")

    while True:
        try:
            async with httpx.AsyncClient(timeout=300.0) as client:  # 5 minute timeout
                # 1. Poll for failed jobs
                logger.info(f"[Active Mode] Polling for failed jobs...")
                resp = await client.get(
                    f"{BACKEND_API_URL}/v1/analysis/failed",
                    params={"limit": 3}
                )

                if resp.status_code != 200:
                    logger.error(f"[Active Mode] Failed to fetch jobs: {resp.status_code} - {resp.text}")
                    await asyncio.sleep(poll_interval_seconds)
                    continue

                data = resp.json()
                jobs = data.get('jobs', [])

                if not jobs:
                    logger.info("[Active Mode] No failed jobs found")
                    await asyncio.sleep(poll_interval_seconds)
                    continue

                logger.info(f"[Active Mode] Found {len(jobs)} failed jobs")

                # 2. Process each job
                for job in jobs:
                    job_id = job['job_id']
                    try:
                        # Process the job
                        result = await process_job(job)

                        # 3. Submit result back to backend
                        logger.info(f"[Active Mode] Submitting result for job {job_id}")
                        submit_resp = await client.put(
                            f"{BACKEND_API_URL}/v1/analysis/{job_id}/result",
                            json=result
                        )

                        if submit_resp.status_code == 200:
                            logger.info(f"[Active Mode] ✅ Job {job_id} completed successfully")
                        else:
                            logger.error(
                                f"[Active Mode] ❌ Failed to submit result for job {job_id}: "
                                f"{submit_resp.status_code} - {submit_resp.text}"
                            )

                    except Exception as e:
                        logger.error(f"[Active Mode] ❌ Job {job_id} failed: {str(e)}", exc_info=True)
                        # Continue to next job
                        continue

        except Exception as e:
            logger.error(f"[Active Mode] Polling error: {str(e)}", exc_info=True)

        # Wait before next poll
        await asyncio.sleep(poll_interval_seconds)


if __name__ == "__main__":
    import uvicorn
    import multiprocessing
    from .config import HOST, PORT, UVICORN_WORKERS

    # Calculate workers based on CPU cores if not specified
    cpu_count = multiprocessing.cpu_count()
    workers = UVICORN_WORKERS if UVICORN_WORKERS > 0 else cpu_count * 2

    logger.info(f"Starting 愛煮小幫手 Video Processor on {HOST}:{PORT}")
    logger.info(f"Gemini API key configured: {bool(GEMINI_API_KEY)}")
    logger.info(f"CPU cores detected: {cpu_count}, starting {workers} workers")

    # Use import string format to enable workers functionality
    uvicorn.run(
        "src.main:app",
        host=HOST,
        port=PORT,
        workers=workers
    )
